# -*- coding: utf-8 -*-
"""MAIS 202 project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1afAYR8wq6yuO2K54oF9d-jI00dRAAW_k

# Omar, Abderrahman and Tahsin's Final Project

Project statement - our project consists of a language identification program that will detect the language of a text. The languages that the program will detect are the 235 languages Wikipedia articles are written in. We will use a Naive Bayes algorithm to implement this.

1. Data preprocessing:
"""

# importing libraries
import pandas as pd
from collections import Counter
import string
import re

# Hyperparameters
most_common_languages = ['eng', 'deu', 'hin', 'spa', 'fra', 'ara', 'ben', 'rus',
                         'por', 'urd']
word_per_lang = 300


# define cleaning function
def clean(text):
    clean_string = text.lower()  # make lower case
    clean_string = clean_string.translate(str.maketrans('', '', string.punctuation))  # remove punctuation
    pattern = r'[0-9]'
    clean_string = re.sub(pattern, '', clean_string)
    clean_string = re.sub(r'[\\\\/:*«`\'?¿";!<>,.|()-_)(}{#$%@^&~+-=–—‘’“”„†•…′ⁿ№−、《》「」『』（），－：；] ° »/():–".،', '',
                          text.lower().strip())
    return clean_string


# load in data labels and training data

X_train_file = open('../wili-2018/x_train.txt', 'r')
Y_train_file = open('../wili-2018/y_train.txt', 'r')
X_train_str = X_train_file.read()
Y_train_str = Y_train_file.read()

X_train = X_train_str.split("\n")
Y_train = Y_train_str.split("\n")

# reduce data
X_train_10 = []
Y_train_10 = []
for i in range(len(X_train)):
    language = Y_train[i]
    if language in most_common_languages:
        X_train_10 = X_train_10 + [clean(X_train[i])]
        Y_train_10 = Y_train_10 + [language]

print("Length of X_train: " + str(len(X_train_10)))

# create dictionary of text of all languages
# with each key being a language and the values being every sentence in that languagae 

lang_dict = {}
for lang in most_common_languages:
    lang_dict[lang] = []
#print(lang_dict)

for i in range(len(X_train_10)):
    language = Y_train_10[i]
    try:
        lang_dict[language] = lang_dict[language] + [clean(X_train_10[i])]
    except:
        print(language)

#print(lang_dict["eng"][0:2])

# create a dictionary with language mapping to all words in the language
vocab_dict = {}
for lang in most_common_languages:
    vocab_dict[lang] = []

# for each language, first join the list of 
# sentences then split to create a list of all words, then find all unique words 

for language in lang_dict:  # iterate through dictionary, for each language,
    # join list into giant string
    words = " ".join(lang_dict[language])
    # split words into list
    list_words = words.split()
    # append list of words into vocab dict
    vocab_dict[language] = list_words

print(vocab_dict["eng"])

"""We will proceed now by implementing the bag-of-words method that takes a language and returns a list of the most common words in it."""


# Bag of words
def bag_of_words(text, bag_size):
    return [c[0] for c in Counter(text).most_common(bag_size)]


bag = []

for lang in most_common_languages:
    bag = bag + bag_of_words(vocab_dict[lang], word_per_lang)

print("Bag: " + str(len(bag)))
print(bag)


# vectorize a certain piece of text

def vectorize(sentence, bag):
    return_dict = [0] * word_per_lang * 10
    count = 0
    for i in [bag.index(word) for word in sentence.split() if word in bag]:
        return_dict[i] += 1
        count += 1

    # print(count)
    return return_dict


# Vectorize the x train

X_train_vect = []
for i in X_train_10:
    X_train_vect.append(vectorize(i, bag))

print(max(X_train_vect[2]))

"""Implementation of Naive Bayes"""

# get test data 

# load in data labels and training data 

X_test_file = open('../wili-2018/x_test.txt', 'r')
Y_test_file = open('../wili-2018/y_test.txt', 'r')
X_test_str = X_test_file.read()
Y_test_str = Y_test_file.read()

X_test = X_test_str.split("\n")
Y_test = Y_test_str.split("\n")


X_test_10 = []
Y_test_10 = []
for i in range(len(X_test)):
    language = Y_test[i]
    if language in most_common_languages:
        X_test_10 = X_test_10 + [clean(X_test[i])]
        Y_test_10 = Y_test_10 + [language]

print("Length of X_test: " + str(len(X_test_10)))

# vectorize test
X_test_vect = []
for i in X_test_10:
    X_test_vect.append(vectorize(i, bag))

# fit model and test accuracy 
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print("Start to Train Naive Bayes")
# classify through multinomial naive bayes and fit the data accordingly 
clf = MultinomialNB()
clf.fit(X_train_vect, Y_train_10)

# predict languages languages 
Y_train_pred = clf.predict(X_train_vect)
Y_test_pred = clf.predict(X_test_vect)

# print accuracy scores
print("Training accuracy: ", accuracy_score(Y_train_pred, Y_train_10))
print("Test accuracy: ", accuracy_score(Y_test_pred, Y_test_10))

# plot confusion matrix
ConfusionMatrixDisplay.from_predictions(
    Y_test_10, Y_test_pred)
plt.show()

# Random forest 
from sklearn.ensemble import RandomForestClassifier

print("Start to Train Random Forest")
rfc = RandomForestClassifier(n_estimators=500, min_samples_split=60, max_depth=55, max_features=80)
rfc.fit(X_train_vect, Y_train_10)
print(accuracy_score(rfc.predict(X_train_vect), Y_train_10))
print(accuracy_score(rfc.predict(X_test_vect), Y_test_10))

ConfusionMatrixDisplay.from_predictions(
    Y_test_10, rfc.predict(X_test_vect))
plt.show()

from sklearn import svm

print("Start to Train SVM")
svm_clf = svm.LinearSVC(max_iter=10000)
svm_clf.fit(X_train_vect, Y_train_10)
print(accuracy_score(svm_clf.predict(X_train_vect), Y_train_10))
print(accuracy_score(svm_clf.predict(X_test_vect), Y_test_10))

'''
for i in range(len(Y_train_10)) : 
  print("language: ", Y_train_10[i])
  print("accuracy: ", accuracy_score(svm_clf.predict(X_train_vect), Y_train_10[i]))
'''

ConfusionMatrixDisplay.from_predictions(
    Y_test_10, svm_clf.predict(X_test_vect))
plt.show()


def identify_text(text):
    vector = [vectorize(clean(text), bag)]
    print("SVM: " + svm_clf.predict(vector)[0] + " | Random Forest: " + rfc.predict(vector)[0] + " | Naive Bayes: " +
          clf.predict(vector)[0])


identify_text("Hello! My name is Abderrahman! I like the potatoes ")

identify_text("je suis la personne malade.")

identify_text("Je souhaite soumettre une œuvre VR.")

identify_text("Chaque participant peut soumettre jusqu'à cinq photos par concours.")

identify_text("L'invention constitue donc un dispositif miniaturisé économique.")

identify_text("We should play with legos at camp.")

identify_text("We should play with legos at camp.")

identify_text("There's an art to getting your way, and spitting olive pits across the table isn't it.")

identify_text("It's never been my responsibility to glaze the donuts.")

identify_text("مساء الخير")

identify_text("أراك في المرة القادمة")

identify_text("كيف أصل إلى هناك، من فضلك؟")

identify_text("لا تقلق")

identify_text("أهلاً")

identify_text("এই ভাষা যথেষ্ট নয়")

identify_text("নয়নের মণি")

identify_text("ভালবাসার নৌকা পাহাড় বইয়ে যায়।")

identify_text("কারো সাথে ভাগাভাগি করলে সমস্যা অর্ধেক হয়ে যায়")

identify_text("¡Qué bien te ves!")

identify_text("¡Cuánto tiempo sin verte!")

identify_text("Parece que fue ayer, ¿no?")

identify_text("No me vengas con esto ahora.")

identify_text("¿Conoces algún lugar cerca donde podamos hablar tranquilamente?")

identify_text("Esperame un rato que tengo que reiniciar el modem")
